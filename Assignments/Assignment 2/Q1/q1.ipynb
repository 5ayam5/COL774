{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "K = 5\n",
    "TRAIN_DATA = \"reviews_Digital_Music_5.json/Music_Review_train.json\"\n",
    "TEST_DATA = \"reviews_Digital_Music_5.json/Music_Review_test.json\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def extract_data(file: str, split):\n",
    "\twith open(file) as f:\n",
    "\t\tX = []\n",
    "\t\tY = []\n",
    "\t\tfor line in f:\n",
    "\t\t\tX.append(defaultdict(int))\n",
    "\t\t\tx = json.loads(line)\n",
    "\t\t\tY.append(int(x['overall']))\n",
    "\t\t\tfor word in split(x['reviewText']):\n",
    "\t\t\t\tX[-1][word] += 1\n",
    "\t\treturn np.array(X), np.array(Y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def train_model(X: np.ndarray, Y: np.ndarray):\n",
    "\tvocab = defaultdict(lambda: np.ones(K))\n",
    "\tPhi = np.zeros(K, np.float64)\n",
    "\tn = np.zeros(K, np.int32)\n",
    "\tfor x, y in zip(X, Y):\n",
    "\t\ty -= 1\n",
    "\t\tPhi[y] += 1\n",
    "\t\tfor word, count in x.items():\n",
    "\t\t\tvocab[word][y] += count\n",
    "\t\t\tn[y] += count\n",
    "\tn += len(vocab)\n",
    "\tfor word in vocab:\n",
    "\t\tvocab[word] = np.log(vocab[word] / n)\n",
    "\tn = np.log(1 / (1 + n))\n",
    "\treturn vocab, n, np.log(Phi / np.sum(Phi))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def predict_nb(vocab: defaultdict, n: np.ndarray, Phi: np.ndarray, x: defaultdict):\n",
    "\tpred = -1\n",
    "\tbest = -float(\"inf\")\n",
    "\tfor y, phi in enumerate(Phi):\n",
    "\t\tprob = phi\n",
    "\t\tfor word in x:\n",
    "\t\t\tif word in vocab:\n",
    "\t\t\t\tprob += vocab[word][y] * x[word]\n",
    "\t\t\telse:\n",
    "\t\t\t\tprob += n[y]\n",
    "\t\tif prob > best:\n",
    "\t\t\tbest = prob\n",
    "\t\t\tpred = y + 1\n",
    "\treturn pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def accuracy(X: np.ndarray, Y: np.ndarray, prediction):\n",
    "\treturn sum(prediction(X) == Y) / Y.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def confusion_matrix(Y: np.ndarray, pred_Y: np.ndarray):\n",
    "\tconfusion = np.zeros((K, K), np.int32)\n",
    "\tfor y, pred_y in zip(Y, pred_Y):\n",
    "\t\tconfusion[y - 1][pred_y - 1] += 1\n",
    "\treturn confusion"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def gen_train_test(split_fn):\n",
    "\tprint(\"Extracting data...\")\n",
    "\ttraining_data = extract_data(TRAIN_DATA, split_fn)\n",
    "\ttest_data = extract_data(TEST_DATA, split_fn)\n",
    "\tprint(\"Data extracted!\")\n",
    "\treturn training_data, test_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def nb_util(training_data, test_data, output, extra=False):\n",
    "\tprint(\"Training model...\")\n",
    "\tvocab, n, Phi = train_model(*training_data)\n",
    "\tprint(\"Model trained!\\nMaking predictions and writing output to file...\")\n",
    "\tnaive_bayes = np.vectorize(lambda x: predict_nb(vocab, n, Phi, x))\n",
    "\tm = test_data[1].shape[0]\n",
    "\ttraining_pred = naive_bayes(training_data[0])\n",
    "\ttest_pred = naive_bayes(test_data[0])\n",
    "\twith open(output, 'w+') as f:\n",
    "\t\tf.write(\"train_accuracy   = {}\\n\".format(accuracy(*training_data, lambda X: training_pred)))\n",
    "\t\tf.write(\"test_accuracy    = {}\\n\".format(accuracy(*test_data, lambda X: test_pred)))\n",
    "\t\tif extra:\n",
    "\t\t\tf.write(\"random_accuracy  = {}\\n\".format(accuracy(*test_data, lambda X: np.random.randint(1, 6, m))))\n",
    "\t\t\tf.write(\"mode_accuracy    = {}\\n\".format(accuracy(*test_data, lambda X: np.full(m, Counter(test_data[1]).most_common(1)[0][0]))))\n",
    "\t\t\tf.write(\"confusion_matrix (training) =\\n{}\\n\".format(confusion_matrix(training_data[1], training_pred)))\n",
    "\t\tf.write(\"confusion_matrix (test) =\\n{}\".format(confusion_matrix(test_data[1], test_pred)))\n",
    "\tprint(\"Output written!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "porter = nltk.stem.PorterStemmer()\n",
    "stemmed = dict()\n",
    "def stem(word: str):\n",
    "\tif word not in stemmed:\n",
    "\t\tstemmed[word] = porter.stem(word)\n",
    "\treturn stemmed[word]\n",
    "def stem_split(s: str):\n",
    "\traw = nltk.tokenize.word_tokenize(re.sub('[{}]'.format(string.punctuation), ' ', s))\n",
    "\tstop_and_stem = []\n",
    "\tfor word in raw:\n",
    "\t\tif not word in stop_words:\n",
    "\t\t\tstop_and_stem.append(stem(word))\n",
    "\treturn stop_and_stem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def bigram_split(s: str):\n",
    "\tstemmed = stem_split(s)\n",
    "\tret = []\n",
    "\tfor i in range(len(stemmed) - 1):\n",
    "\t\tret.append(stemmed[i] + stemmed[i + 1])\n",
    "\treturn ret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def bigram_split_alter(s: str):\n",
    "\tsplit_text = re.sub('[{}]'.format(string.punctuation), ' ', s).split()\n",
    "\tret = []\n",
    "\tfor i in range(len(split_text) - 1):\n",
    "\t\tret.append(split_text[i] + split_text[i + 1])\n",
    "\treturn ret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def trigram_split(s: str):\n",
    "\tstemmed = stem_split(s)\n",
    "\tret = []\n",
    "\tfor i in range(len(stemmed) - 2):\n",
    "\t\tret.append(stemmed[i] + stemmed[i + 1] + stemmed[i + 2])\n",
    "\treturn ret"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "training_default, test_default = gen_train_test(str.split)\n",
    "nb_util(training_default, test_default, \"output/default\", True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data...\n",
      "Data extracted!\n",
      "Training model...\n",
      "Model trained!\n",
      "Making predictions and writing output to file...\n",
      "Output written!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "training_clean, test_clean = gen_train_test(stem_split)\n",
    "nb_util(training_clean, test_clean, \"output/clean\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data...\n",
      "Data extracted!\n",
      "Training model...\n",
      "Model trained!\n",
      "Making predictions and writing output to file...\n",
      "Output written!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "training_bigram_clean, test__bigram_clean = gen_train_test(bigram_split)\n",
    "nb_util(training_bigram_clean, test__bigram_clean, \"output/bigram\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data...\n",
      "Data extracted!\n",
      "Training model...\n",
      "Model trained!\n",
      "Making predictions and writing output to file...\n",
      "Output written!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "training_bigram, test_bigram = gen_train_test(bigram_split_alter)\n",
    "nb_util(training_bigram, test_bigram, \"output/bigram_og\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data...\n",
      "Data extracted!\n",
      "Training model...\n",
      "Model trained!\n",
      "Making predictions and writing output to file...\n",
      "Output written!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "training_trigram_clean, test__trigram_clean = gen_train_test(trigram_split)\n",
    "nb_util(training_trigram_clean, test__trigram_clean, \"output/trigram\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting data...\n",
      "Data extracted!\n",
      "Training model...\n",
      "Model trained!\n",
      "Making predictions and writing output to file...\n",
      "Output written!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "c88bcd8217886c4a06b3653a3d64d637eca0a96d4c4031ee5dfe8cf426268753"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}